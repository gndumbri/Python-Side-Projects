{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PharmCAS Web Crawler",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4kThTAscpMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt install chromium-chromedriver\n",
        "!pip install selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import files\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soqxZWqdvaUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "driver = webdriver.Chrome('chromedriver',options=options)\n",
        "driver.get(\"http://www.pharmcas.org/school-directory/#/pharmd/general-information\")\n",
        "driver.implicitly_wait(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S19D1_01vrG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numbers = driver.find_elements(By.XPATH, \n",
        "          '//table[@class=\"phc-schools-table phc-schools-table-pharmd\"]//a')\n",
        "links = []\n",
        "for n in numbers:\n",
        "    number = n.get_attribute('href')\n",
        "    links.append(number)\n",
        "\n",
        "len(links)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXgonq9kyj1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pharm_crawl(a_list, data_list):\n",
        "    headers = {'user-agent': 'jobscraper - school project (gndumbri@gmail.com)'}\n",
        "    for i in a_list:\n",
        "        url = i\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.ok:\n",
        "            data = response.text\n",
        "\n",
        "            soup = BeautifulSoup(data, 'html.parser')\n",
        "\n",
        "            alert = soup.select('aside:nth-of-type(1)')\n",
        "            alerts = [al.get_text() for al in alert]\n",
        "            all_alerts = ''.join(alerts).strip()\n",
        "            all_alerts = all_alerts.replace('\\n                        ', '')\n",
        "            \n",
        "            name = soup.select('h1')\n",
        "            names = [nom.get_text() for nom in name]\n",
        "            all_names = ''.join(names[0]).strip()\n",
        "\n",
        "            if soup.find(\"span\", itemprop=\"addressRegion\") is not None:\n",
        "                state = soup.find(\"span\", itemprop=\"addressRegion\").get_text()\n",
        "            else:\n",
        "                state = 'N/A'\n",
        "\n",
        "            deadline = soup.select('.deadline > table > tbody > tr > \\\n",
        "            td:nth-of-type(2)')\n",
        "            deadlines = [dead.get_text() for dead in deadline]\n",
        "            all_deadlines = ''.join(deadlines)\n",
        "\n",
        "            seat = soup.select('.prog-statistics > table > tbody > \\\n",
        "            tr:nth-of-type(1) > td:nth-of-type(2)')\n",
        "            seats = [spot.get_text() for spot in seat]\n",
        "            all_seats = ''.join(seats)\n",
        "\n",
        "            update = soup.select('aside:nth-of-type(2)')\n",
        "            updates = [up.get_text() for up in update]\n",
        "            all_updates = ''.join(updates).strip()\n",
        "\n",
        "            private = soup.select('.prog-information > table > tbody > \\\n",
        "            tr:nth-of-type(3) > td:nth-of-type(2)')\n",
        "            statuses = [status.get_text() for status in private]\n",
        "            all_statuses = ''.join(statuses).strip()\n",
        "\n",
        "            early = soup.select('.prog-information > table > tbody > \\\n",
        "            tr:nth-of-type(11) > td:nth-of-type(2)')\n",
        "            early_dec = [first.get_text() for first in early]\n",
        "            all_early = ''.join(early_dec)\n",
        "            \n",
        "            ea = soup.select('.prog-information > table > tbody > \\\n",
        "            tr:nth-of-type(12) > td:nth-of-type(2) > ul > li')\n",
        "            eas = [e_a.get_text() for e_a in ea]\n",
        "            all_eas = ''.join(eas)\n",
        "\n",
        "            min_ovr = soup.select('.prog-criteria > table > tbody > \\\n",
        "            tr:nth-of-type(1) > td:nth-of-type(2)')\n",
        "            mins = [ovr.get_text() for ovr in min_ovr]\n",
        "            all_min_ovr = ''.join(mins)\n",
        "\n",
        "            min_req = soup.select('.prog-criteria > table > tbody > \\\n",
        "            tr:nth-of-type(2) > td:nth-of-type(2)')\n",
        "            reqs = [req.get_text() for req in min_req]\n",
        "            all_min_req = ''.join(reqs)\n",
        "\n",
        "            hour = soup.select('.prog-prerequisites > table:nth-of-type(1) > \\\n",
        "            tbody > tr:nth-of-type(1) > td:nth-of-type(2)')\n",
        "            hours = [our.get_text() for our in hour]\n",
        "            all_hours = ''.join(hours)\n",
        "\n",
        "            pcat = soup.select('.test > table > tbody > tr:nth-of-type(1) > \\\n",
        "            td:nth-of-type(2) > p')\n",
        "            scores = [score.get_text() for score in pcat]\n",
        "            all_scores = ''.join(scores).strip()\n",
        "\n",
        "            pcat_score = soup.select('.test > table > tbody > \\\n",
        "            tr:nth-of-type(2) > td:nth-of-type(2)')\n",
        "            pcat_scores = [pcs.get_text() for pcs in pcat_score]\n",
        "            all_pcat_scores = ''.join(pcat_scores)\n",
        "\n",
        "            lor = soup.select('.letters-of-reference > table > tbody > \\\n",
        "            tr:nth-of-type(2) > td:nth-of-type(2)')\n",
        "            lors = [lo.get_text() for lo in lor]\n",
        "            all_lors = ''.join(lors).strip()\n",
        "\n",
        "            deposit = soup.select('.accepted-applicants > table > tbody > tr > \\\n",
        "            td:nth-of-type(2), table > tbody > tr')\n",
        "            deposits = [dep.get_text() for dep in deposit]\n",
        "            all_deps = ((''.join(deposits[0])).replace('\\n', ' '))\n",
        "\n",
        "        data_list.append((all_alerts, all_names, state, all_deadlines, \n",
        "                          all_seats, all_updates, all_statuses, \n",
        "                          all_early, all_eas, all_min_ovr, all_min_req, \n",
        "                          all_hours, all_scores, all_pcat_scores, \n",
        "                          all_lors, all_deps))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uktE43blRwyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pharm_crawl(links[11:], data_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWDHl_Eq3cXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(data_list, columns=['Alerts','School', \n",
        "                  'State', 'Application Deadline', \n",
        "                  'Class Size', 'Updates', \n",
        "                  'Inst Type', 'Early Decision',\n",
        "                  'Specialty Programs', 'Min Overall GPA',\n",
        "                  'Min Pre-Req GPA', 'Pre-req hours',\n",
        "                  'PCAT Required', 'Minimum Composite PCAT Score Considered', \n",
        "                  'LORs', 'Deposit'])\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zmDvHLfmbao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('Pharmacy_School_Surveillance_Program.csv', index = None, \n",
        "          header=True, encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDj9nQY3qm_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('Pharmacy_School_Surveillance_Program.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}